//
//  AudioRecorderManager.swift
//  whispr
//
//  Created by åˆ˜æ²›å¼º on 2025/7/24.
//

import AVFoundation
import Foundation
import SwiftUI

class AudioRecorderManager: NSObject, ObservableObject {
    @Published var isRecording = false
    @Published var hasPermission = false
    @Published var permissionStatus: String = "æœªçŸ¥"

    private var audioRecorder: AVAudioRecorder?
    private var audioEngine: AVAudioEngine?
    private var inputNode: AVAudioInputNode?
    private let audioSession = AVAudioSession.sharedInstance()

    // Deepgram WebSocket ç®¡ç†å™¨ - å…¬å¼€è®¿é—®
    let transcriptionManager = DeepgramTranscriptionManager()

    override init() {
        super.init()
        checkPermissionStatus()
        setupAudioEngine()
    }

    // MARK: - Audio Engine Setup

    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        inputNode = audioEngine?.inputNode
    }

    func checkPermissionStatus() {
        switch AVAudioSession.sharedInstance().recordPermission {
        case .granted:
            hasPermission = true
            permissionStatus = "å·²æˆæƒ"
        case .denied:
            hasPermission = false
            permissionStatus = "å·²æ‹’ç»"
        case .undetermined:
            hasPermission = false
            permissionStatus = "æœªç¡®å®š"
        @unknown default:
            hasPermission = false
            permissionStatus = "æœªçŸ¥"
        }
    }

    func requestPermission() {
        AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
            DispatchQueue.main.async {
                self?.hasPermission = granted
                self?.permissionStatus = granted ? "å·²æˆæƒ" : "å·²æ‹’ç»"
                // æƒé™å˜æ›´åé‡æ–°æ£€æŸ¥çŠ¶æ€
                self?.checkPermissionStatus()
            }
        }
    }

    func startRecording() {
        guard hasPermission else {
            requestPermission()
            return
        }

        do {
            try audioSession.setCategory(.playAndRecord, mode: .measurement)
            try audioSession.setActive(
                true,
                options: .notifyOthersOnDeactivation
            )

            // è¿æ¥åˆ° OpenAI WebSocket
            transcriptionManager.connectToDeepgram()

            // å¯åŠ¨éŸ³é¢‘å¼•æ“è¿›è¡Œå®æ—¶æµä¼ è¾“
            try startAudioStreaming()

            // åŒæ—¶å¯åŠ¨æ–‡ä»¶å½•éŸ³ï¼ˆå¯é€‰ï¼‰
            try startFileRecording()

            isRecording = true
            print("âœ… å¼€å§‹å½•éŸ³å’Œå®æ—¶è½¬å½•")

        } catch {
            print("âŒ å½•éŸ³å¤±è´¥: \(error.localizedDescription)")
        }
    }

    private func startAudioStreaming() throws {
        guard let audioEngine = audioEngine,
            let inputNode = inputNode
        else {
            throw NSError(
                domain: "AudioEngine",
                code: 1,
                userInfo: [NSLocalizedDescriptionKey: "éŸ³é¢‘å¼•æ“æœªåˆå§‹åŒ–"]
            )
        }

        // è·å–è¾“å…¥æ ¼å¼ï¼Œä½†è¦æ£€æŸ¥æœ‰æ•ˆæ€§
        let inputFormat = inputNode.outputFormat(forBus: 0)
        print(
            "ğŸ¤ è¾“å…¥æ ¼å¼: é‡‡æ ·ç‡=\(inputFormat.sampleRate), å£°é“æ•°=\(inputFormat.channelCount)"
        )

        // åˆ›å»ºä¸€ä¸ªæœ‰æ•ˆçš„è¾“å…¥æ ¼å¼ï¼ˆå¦‚æœåŸå§‹æ ¼å¼æ— æ•ˆï¼‰
        let validInputFormat: AVAudioFormat
        if inputFormat.sampleRate == 0 || inputFormat.channelCount == 0 {
            guard
                let defaultFormat = AVAudioFormat(
                    commonFormat: .pcmFormatFloat32,
                    sampleRate: 48000,
                    channels: 1,
                    interleaved: false
                )
            else {
                throw NSError(
                    domain: "AudioFormat",
                    code: 1,
                    userInfo: [NSLocalizedDescriptionKey: "æ— æ³•åˆ›å»ºé»˜è®¤è¾“å…¥éŸ³é¢‘æ ¼å¼"]
                )
            }
            validInputFormat = defaultFormat
            print(
                "âš ï¸ ä½¿ç”¨é»˜è®¤è¾“å…¥æ ¼å¼: é‡‡æ ·ç‡=\(validInputFormat.sampleRate), å£°é“æ•°=\(validInputFormat.channelCount)"
            )
        } else {
            validInputFormat = inputFormat
        }

        // åˆ›å»ºä¸­é—´æ ¼å¼ - ä¸è¾“å…¥æ ¼å¼å…¼å®¹ï¼Œç”¨äº tap
        guard
            let intermediateFormat = AVAudioFormat(
                commonFormat: .pcmFormatFloat32,
                sampleRate: validInputFormat.sampleRate, // ä¿æŒè¾“å…¥é‡‡æ ·ç‡
                channels: validInputFormat.channelCount,  // ä¿æŒè¾“å…¥å£°é“æ•°
                interleaved: false
            )
        else {
            throw NSError(
                domain: "AudioFormat",
                code: 1,
                userInfo: [NSLocalizedDescriptionKey: "æ— æ³•åˆ›å»ºä¸­é—´éŸ³é¢‘æ ¼å¼"]
            )
        }

        print(
            "ğŸµ ä¸­é—´æ ¼å¼: é‡‡æ ·ç‡=\(intermediateFormat.sampleRate), å£°é“æ•°=\(intermediateFormat.channelCount)"
        )

        let converterNode = AVAudioMixerNode()
        let sinkNode = AVAudioMixerNode()

        audioEngine.attach(converterNode)
        audioEngine.attach(sinkNode)

        // åœ¨ converterNode ä¸Šå®‰è£… tapï¼Œä½¿ç”¨å…¼å®¹çš„ä¸­é—´æ ¼å¼
        converterNode.installTap(
            onBus: 0,
            bufferSize: 1024,
            format: intermediateFormat // ä½¿ç”¨ä¸è¾“å…¥å…¼å®¹çš„æ ¼å¼
        ) { [weak self] buffer, time in
            // åœ¨è¿™é‡Œè¿›è¡Œæ ¼å¼è½¬æ¢åˆ° Deepgram è¦æ±‚çš„æ ¼å¼
            if let convertedData = self?.convertBufferToDeepgramFormat(buffer) {
                self?.transcriptionManager.sendAudioData(convertedData)
            }
        }

        // è¿æ¥éŸ³é¢‘èŠ‚ç‚¹
        audioEngine.connect(
            inputNode,
            to: converterNode,
            format: validInputFormat
        )
        audioEngine.connect(converterNode, to: sinkNode, format: intermediateFormat)

        print("ğŸ”— éŸ³é¢‘èŠ‚ç‚¹è¿æ¥å®Œæˆ")
        audioEngine.prepare()

        // å¯åŠ¨éŸ³é¢‘å¼•æ“
        try audioEngine.start()
        print("ğŸ™ï¸ Deepgram éŸ³é¢‘æµä¼ è¾“å·²å¼€å§‹")
    }

    // æ–°å¢ï¼šå°†éŸ³é¢‘ç¼“å†²åŒºè½¬æ¢ä¸º Deepgram è¦æ±‚çš„æ ¼å¼
    private func convertBufferToDeepgramFormat(_ inputBuffer: AVAudioPCMBuffer) -> Data? {
        // åˆ›å»º Deepgram è¦æ±‚çš„è¾“å‡ºæ ¼å¼
        guard let outputFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: 16000,
            channels: 1,
            interleaved: true
        ) else {
            print("âŒ æ— æ³•åˆ›å»º Deepgram è¾“å‡ºæ ¼å¼")
            return nil
        }

        // åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨
        guard let converter = AVAudioConverter(from: inputBuffer.format, to: outputFormat) else {
            print("âŒ æ— æ³•åˆ›å»ºéŸ³é¢‘è½¬æ¢å™¨")
            return nil
        }

        // è®¡ç®—è¾“å‡ºç¼“å†²åŒºå¤§å°
        let outputCapacity = AVAudioFrameCount(
            Double(inputBuffer.frameLength) * (outputFormat.sampleRate / inputBuffer.format.sampleRate)
        )

        guard let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat, frameCapacity: outputCapacity) else {
            print("âŒ æ— æ³•åˆ›å»ºè¾“å‡ºç¼“å†²åŒº")
            return nil
        }

        var error: NSError?
        let status = converter.convert(to: outputBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return inputBuffer
        }

        if status == .error {
            print("âŒ éŸ³é¢‘è½¬æ¢å¤±è´¥: \(error?.localizedDescription ?? "æœªçŸ¥é”™è¯¯")")
            return nil
        }

        // è½¬æ¢ä¸º Data
        return toNSData(buffer: outputBuffer)
    }

    // å‚è€ƒ UIKit ä»£ç çš„æ•°æ®è½¬æ¢æ–¹æ³•
    private func toNSData(buffer: AVAudioPCMBuffer) -> Data? {
        let audioBuffer = buffer.audioBufferList.pointee.mBuffers
        return Data(
            bytes: audioBuffer.mData!,
            count: Int(audioBuffer.mDataByteSize)
        )
    }

    private func startFileRecording() throws {
        let documentsPath = FileManager.default.urls(
            for: .documentDirectory,
            in: .userDomainMask
        )[0]
        let audioFilename = documentsPath.appendingPathComponent(
            "recording_\(Date().timeIntervalSince1970).m4a"
        )

        let settings = [
            AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
            AVSampleRateKey: 44100,
            AVNumberOfChannelsKey: 2,
            AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue,
        ]

        audioRecorder = try AVAudioRecorder(
            url: audioFilename,
            settings: settings
        )
        audioRecorder?.record()

        print("ğŸ“ æ–‡ä»¶å½•éŸ³å·²å¼€å§‹: \(audioFilename)")
    }

    func stopRecording() {
        // åœæ­¢éŸ³é¢‘å¼•æ“
        audioEngine?.stop()
        inputNode?.removeTap(onBus: 0)

        // åœæ­¢æ–‡ä»¶å½•éŸ³
        audioRecorder?.stop()

        // æ–­å¼€ WebSocket è¿æ¥
        transcriptionManager.disconnect()

        isRecording = false

        do {
            try audioSession.setActive(false)
            print("âœ… å½•éŸ³å’Œè½¬å½•å·²åœæ­¢")
        } catch {
            print("âŒ åœæ­¢å½•éŸ³æ—¶å‡ºé”™: \(error.localizedDescription)")
        }
    }

    func toggleRecording() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
}
